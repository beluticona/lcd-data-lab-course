{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase 10: Selección y preparacion de features.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNm-dDcDWYET"
      },
      "source": [
        "# Más sobre evaluación de modelos y selección de features (tercera parte)\n",
        "\n",
        "En este tercer notebook vamos a buscar responder la siguiente pregunta: **¿qué datos me conviene incorporar como features de mi modelo?**\n",
        "\n",
        "Seguimos trabajando con los datos de INTA, así que repetimos el proceso para cargarlos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTujCTikWGp6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "45055bf6-0043-45d6-e5fd-ffcd76f31dd9"
      },
      "source": [
        "# Como siempre, tratamos de traer todos los paquetes al ppio\n",
        "from google.colab import drive # Para montar nuestro drive en la consola\n",
        "import matplotlib.pylab as plt # Para gráficos\n",
        "import numpy as np # Para manejo de arrays, operaciones matemáticas, etc.\n",
        "from sklearn.linear_model import LogisticRegression # El método de regresión logística que vamos a usar\n",
        "import pandas as pd # Para manejo de base de datos\n",
        "\n",
        "# Traemos los datos\n",
        "drive.mount('/content/drive') # Montamos nuestra unidad de Google Drive\n",
        "\n",
        "filename = '/content/drive/My Drive/LaboDatos2021/datosDiariosSanFernandoINTA.xls'\n",
        "\n",
        "d = pd.read_excel(filename) # Levantamos los datos, en este caso, con el método pd.read_excel\n",
        "d.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fecha</th>\n",
              "      <th>Temperatura_Abrigo_150cm</th>\n",
              "      <th>Temperatura_Abrigo_150cm_Maxima</th>\n",
              "      <th>Temperatura_Abrigo_150cm_Minima</th>\n",
              "      <th>Temperatura_Intemperie_5cm_Minima</th>\n",
              "      <th>Temperatura_Intemperie_50cm_Minima</th>\n",
              "      <th>Temperatura_Suelo_5cm_Media</th>\n",
              "      <th>Temperatura_Suelo_10cm_Media</th>\n",
              "      <th>Temperatura_Inte_5cm</th>\n",
              "      <th>Temperatura_Intemperie_150cm_Minima</th>\n",
              "      <th>Humedad_Suelo</th>\n",
              "      <th>Precipitacion_Pluviometrica</th>\n",
              "      <th>Precipitacion_Cronologica</th>\n",
              "      <th>Precipitacion_Maxima_30minutos</th>\n",
              "      <th>Heliofania_Efectiva</th>\n",
              "      <th>Heliofania_Relativa</th>\n",
              "      <th>Tesion_Vapor_Media</th>\n",
              "      <th>Humedad_Media</th>\n",
              "      <th>Humedad_Media_8_14_20</th>\n",
              "      <th>Rocio_Medio</th>\n",
              "      <th>Duracion_Follaje_Mojado</th>\n",
              "      <th>Velocidad_Viento_200cm_Media</th>\n",
              "      <th>Direccion_Viento_200cm</th>\n",
              "      <th>Velocidad_Viento_1000cm_Media</th>\n",
              "      <th>Direccion_Viento_1000cm</th>\n",
              "      <th>Velocidad_Viento_Maxima</th>\n",
              "      <th>Presion_Media</th>\n",
              "      <th>Radiacion_Global</th>\n",
              "      <th>Horas_Frio</th>\n",
              "      <th>Unidades_Frio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-02-08 00:00:00.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>79.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-02-09 00:00:00.0</td>\n",
              "      <td>23.80764</td>\n",
              "      <td>28.6</td>\n",
              "      <td>20.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26.21042</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.75</td>\n",
              "      <td>11.75</td>\n",
              "      <td>5.75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19.25281</td>\n",
              "      <td>66.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>16.462780</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.332639</td>\n",
              "      <td>C</td>\n",
              "      <td>1.665799</td>\n",
              "      <td>C</td>\n",
              "      <td>19.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-23.904030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-02-10 00:00:00.0</td>\n",
              "      <td>24.51389</td>\n",
              "      <td>31.7</td>\n",
              "      <td>19.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26.45972</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>23.28392</td>\n",
              "      <td>72.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>18.622350</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.638195</td>\n",
              "      <td>C</td>\n",
              "      <td>2.047743</td>\n",
              "      <td>C</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-23.904030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-02-11 00:00:00.0</td>\n",
              "      <td>19.50139</td>\n",
              "      <td>24.2</td>\n",
              "      <td>15.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24.66320</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.67725</td>\n",
              "      <td>65.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>12.071650</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.247224</td>\n",
              "      <td>E</td>\n",
              "      <td>4.059030</td>\n",
              "      <td>C</td>\n",
              "      <td>20.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-15.106020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-02-12 00:00:00.0</td>\n",
              "      <td>16.70625</td>\n",
              "      <td>24.6</td>\n",
              "      <td>9.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22.63611</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.13446</td>\n",
              "      <td>66.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>9.627476</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.027778</td>\n",
              "      <td>C</td>\n",
              "      <td>1.284722</td>\n",
              "      <td>C</td>\n",
              "      <td>13.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.394994</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Fecha  Temperatura_Abrigo_150cm  ...  Horas_Frio  Unidades_Frio\n",
              "0  2018-02-08 00:00:00.0                       NaN  ...         NaN            NaN\n",
              "1  2018-02-09 00:00:00.0                  23.80764  ...         0.0     -23.904030\n",
              "2  2018-02-10 00:00:00.0                  24.51389  ...         0.0     -23.904030\n",
              "3  2018-02-11 00:00:00.0                  19.50139  ...         0.0     -15.106020\n",
              "4  2018-02-12 00:00:00.0                  16.70625  ...         0.0      -5.394994\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff6MSZfVW9Lz"
      },
      "source": [
        "Luego, filtramos el dataframe descartando columnas con datos faltantes, seleccionamos un subconjunto de columnas, y las renombramos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTay790UXBEJ"
      },
      "source": [
        "vars = ['Precipitacion_Pluviometrica','Temperatura_Abrigo_150cm',\t'Temperatura_Abrigo_150cm_Maxima',\t'Temperatura_Abrigo_150cm_Minima', 'Temperatura_Suelo_10cm_Media', 'Tesion_Vapor_Media',\t'Humedad_Media', 'Humedad_Media_8_14_20',\t'Rocio_Medio', 'Velocidad_Viento_200cm_Media', 'Velocidad_Viento_1000cm_Media', 'Velocidad_Viento_Maxima', 'Horas_Frio',\t'Unidades_Frio']\n",
        "dictvars = {'Precipitacion_Pluviometrica':'precipitacion','Temperatura_Abrigo_150cm':'var1',\t'Temperatura_Abrigo_150cm_Maxima':'var2',\t'Temperatura_Abrigo_150cm_Minima':'var3', 'Temperatura_Suelo_10cm_Media':'var4', 'Tesion_Vapor_Media':'var5',\t'Humedad_Media':'var6', 'Humedad_Media_8_14_20':'var7',\t'Rocio_Medio':'var8', 'Velocidad_Viento_200cm_Media':'var9', 'Velocidad_Viento_1000cm_Media':'var10', 'Velocidad_Viento_Maxima':'var11', 'Horas_Frio':'var12',\t'Unidades_Frio':'var13'}\n",
        "\n",
        "d_filtrado = d[vars].dropna().copy() # Nos quedamos con ciertos campos del data set, para facilitar el trabajo. Y para limitarlo.\n",
        "                                                  # Notar que primero aplicamos el método .dropna() para eliminar filas que tengan alguna columna con NaN \n",
        "                                                  # Ademas, el metodo copy() nos asegura que estemos creando un nuevo dataframe \n",
        "d_filtrado.rename(dictvars, axis = 1, inplace = True)\n",
        "                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gnj818t9XC37"
      },
      "source": [
        "Finalmente, construimos una variable con las etiquetas de los días lluviosos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVZQD9ZdXFjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7d4205-5b96-4900-f806-932d2cf9e15d"
      },
      "source": [
        "d_filtrado['llueveNollueve'] = 0 # empezamos con una columna llena de 0. \n",
        "indice =  d_filtrado['precipitacion'] > 0  # esto me da los valores del indice para los cuales hay precipitacion mayor a 0\n",
        "d_filtrado.loc[indice, 'llueveNollueve'] = 1 # entonces para esos valores del indice pongo 1, porque en el dia correspondiente, llovio\n",
        "\n",
        "d_filtrado['llueveNollueve'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    695\n",
              "1    324\n",
              "Name: llueveNollueve, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ6nWGPz86tN"
      },
      "source": [
        "# Filtros univariados (*select K best*)\n",
        "\n",
        "En un problema típico tenemos varias variables independientes que podemos usar para entrenar nuestro modelo, o sea, incorporar como features. El problema es que no todos los features son igualmente informativos. Es importante partir de una buena base para esperar que un clasificador tenga un buen desempeño.\n",
        "\n",
        "Partamos entonces de un clasificador que incorpora varios features para el problema de predecir los días lluviosos y estimemos el AUC haciendo una validación cruzada estratificada con 5 folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt_sIA119n9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f550b6-9eca-4186-caeb-94fcb2f7a48e"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Definamos la matriz X\n",
        "campos = ['var1','var2','var3','var4','var5','var6','var7','var8','var9','var10','var11','var12','var13'] # Lista que contiene las features de interés.\n",
        "X = d_filtrado[campos].values # En este caso no hace falta reshapear, porque ya tiene la forma que queremos\n",
        "y = np.array(d_filtrado['llueveNollueve']) # Nuestra etiqueta sigue siende la misma de antes\n",
        "\n",
        "X_temp = X\n",
        "for i in np.arange(2,n):\n",
        "    X_temp = np.concatenate((X_temp,X**i), axis=1)\n",
        "X = X_temp\n",
        " \n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5 folds es un número típico si tenemos suficientes datos. Pedimos shuffle=True para que sea al azar la separación en subgrupos\n",
        "skf.get_n_splits(X, y) # arma los folds a partir de los datos\n",
        "\n",
        "auc_values = [] # aca es donde van a ir a parar los AUCs de cada fold\n",
        "\n",
        "for train_index, test_index in skf.split(X, y): # va generando los indices que corresponden a train y test en cada fold\n",
        "    X_train, X_test = X[train_index], X[test_index] # arma que es dato de entrenamiento y qué es dato de evaluación\n",
        "    y_train, y_test = y[train_index], y[test_index]     # idem con los targets\n",
        "\n",
        "    scaler = MinMaxScaler() # escaleo por separado ambos sets\n",
        "    scaler.fit(X_train) \n",
        "    X_train = scaler.transform(X_train)\n",
        "\n",
        "    scaler = MinMaxScaler() # escaleo por separado ambos sets\n",
        "    scaler.fit(X_test) \n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    regLog = LogisticRegression(penalty = 'none', max_iter=10000) # Inicializamos nuevamente el modelo. max_iter es la cantidad de iteraciones maximas del algoritmo de optimizacion de parametros antes de detenerse.\n",
        "    regLog.fit(X_train, y_train) # Ajustamos el modelo con los datos de entrenamiento\n",
        "\n",
        "    probas_test = regLog.predict_proba(X_test)  # probabilidades con datos de evaluación\n",
        "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, probas_test[:,1]) # para plotear curva ROC con datos de entrenamiento\n",
        "    auc_test = roc_auc_score(y_test, probas_test[:,1]) #  AUC con datos de evaluación\n",
        "    auc_values.append(auc_test)\n",
        "print(np.mean(auc_values))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7257280713890426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fePUGMF6AbBB"
      },
      "source": [
        "Ahora supongamos que en cada fold decidimos quedarnos con una cantidad reducida de features, por ejemplo, con los K mejores features de acuerdo a un criterio *univariado*. La idea es que en cada fold tenemos en los datos de entrenamiento las etiquetas de la clase de cada ejemplo. Entonces para cada variable por separado podemos obtener una medida de qué tan distinta es dicha variable entre ambas clases. Entonces, vamos a retener las K variables que más diferentes sean entre clases para entrenar cada clasificador. \n",
        "\n",
        "Para esto, vamos a usar la función usando la estadística F de ANOVA entre grupos para rankear las mejores features **en cada fold por separado.**\n",
        "\n",
        "(si hacemos esto antes de dividir en folds, vamos a estar en problemas, porque habremos usado datos de todo el conjunto para determinar el score F y por lo tanto rankear features, pero entonces habremos pasado datos del conjunto de evaluacion al de entrenamiento cuando dividamos en folds)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PwKgI8vBXXB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "faff6917-dc61-4386-857d-637199cb1a96"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "# Definamos la matriz X\n",
        "campos = ['var1','var2','var3','var4','var5','var6','var7','var8','var9','var10','var11','var12','var13'] # Lista que contiene las features de interés.\n",
        "X = d_filtrado[campos].values # En este caso no hace falta reshapear, porque ya tiene la forma que queremos\n",
        "y = np.array(d_filtrado['llueveNollueve']) # Nuestra etiqueta sigue siende la misma de antes\n",
        "X = np.concatenate((X, X**2,X**3), axis=1) # agrego features elevados a potencias \n",
        "\n",
        "Kbest = 10 # los mejores K que voy a retener\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True) # 5 folds es un número típico si tenemos suficientes datos. Pedimos shuffle=True para que sea al azar la separación en subgrupos\n",
        "skf.get_n_splits(X, y) # arma los folds a partir de los datos\n",
        "\n",
        "auc_values_fs =  []  # aca es donde van a ir a parar los indices de los features seleccionados en cada fold\n",
        "selected_features= np.array([]).reshape(0,X.shape[1]) # aca es donde van a ir a parar los AUCs de cada fold. El reshape es para poder concatenar luego.\n",
        "\n",
        "\n",
        "for train_index, test_index in skf.split(X, y): # va generando los indices que corresponden a train y test en cada fold\n",
        "    X_train, X_test = X[train_index], X[test_index] # arma que es dato de entrenamiento y qué es dato de evaluación\n",
        "    y_train, y_test = y[train_index], y[test_index]     # idem con los targets\n",
        "\n",
        "    scaler = MinMaxScaler() # escaleo por separado ambos sets\n",
        "    scaler.fit(X_train) \n",
        "    X_train = scaler.transform(X_train)\n",
        "\n",
        "    scaler = MinMaxScaler() # escaleo por separado ambos sets\n",
        "    scaler.fit(X_test) \n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    selector = SelectKBest(k=Kbest) # por defecto, usa el F score de ANOVA y los Kbest features\n",
        "    selector.fit(X_train, y_train) # encuentro los F scores \n",
        "    X_train_fs = selector.transform(X_train) # me quedo con los features mejor rankeados en el set de entrenamiento\n",
        "    X_test_fs = selector.transform(X_test) # me quedo con los features mejor rankeados en el set de evaluacion\n",
        "    features = np.array(selector.get_support()).reshape((1,-1)) # esto me pone True si la variable correspondiente fue seleccionada y False sino\n",
        "\n",
        "    selected_features =  np.concatenate((selected_features,features),axis=0)\n",
        "\n",
        "    regLog = LogisticRegression(penalty = 'none', max_iter=10000) # Inicializamos nuevamente el modelo. max_iter es la cantidad de iteraciones maximas del algoritmo de optimizacion de parametros antes de detenerse.\n",
        "    regLog.fit(X_train_fs, y_train) # Ajustamos el modelo con los datos de entrenamiento\n",
        "\n",
        "\n",
        "    probas_test = regLog.predict_proba(X_test_fs)  # probabilidades con datos de evaluación\n",
        "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, probas_test[:,1]) # para plotear curva ROC con datos de entrenamiento\n",
        "    auc_test = roc_auc_score(y_test, probas_test[:,1]) #  AUC con datos de evaluación\n",
        "    auc_values_fs.append(auc_test)\n",
        "\n",
        "print('El AUC promedio es:')\n",
        "print(np.mean(auc_values_fs))\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(np.arange(0,X.shape[1]),np.sum(selected_features,axis=0))\n",
        "plt.title('Seleccion de features')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Folds')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El AUC promedio es:\n",
            "0.7299789014941892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Folds')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Denh6kvgDy-L"
      },
      "source": [
        "Vemos que el AUC promedio obtenido con los K=10 mejores features no es mejor que el AUC obtenido con todos. Esto significa que en este dataset la clasificación no esta basada en un conjunto pequeño de features muy diferentes entre las dos clases, sino que todas parecen contribuir.\n",
        "\n",
        "Más adelante en la materia vamos a ver otros criterios para seleccionar features, denominados recursivos, los cuales permiten tener en cuenta la interacción entre variables, superando en principio a los filtros univariados (pero requiriendo mucha mayor cantidad de cómputos) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ASumH2Ft7c"
      },
      "source": [
        "# Datos categóricos y one-hot encoding\n",
        "\n",
        "Hasta ahora todos los features con los que trabajamos eran numéricos, variables como temperaturas, humedades, etc. ¿Pero qué pasa si tenemos datos categóricos?\n",
        "\n",
        "Consideremos por ejemplo la columna 'Direccion_Viento_200cm' en el dataframe original."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyu0gyghJmRp"
      },
      "source": [
        "d['Direccion_Viento_200cm'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT3c9rRqJtHx"
      },
      "source": [
        "Estas letras reflejan la dirección del viento. No se trata de variables numéricas ni de ordinales (que podemos representar como 1, 2, 3, etc, en virtud de una relación de orden que tienen). Notar ademas que hay datos faltantes que tendriamos que haber removido (tirar los valores donde no hay nada en la variable).\n",
        "\n",
        "Tenemos que pasar esta variable a valores numéricos para que sirva de entrada a nuestro modelo. Pero en este caso, no parece haber ningún motivo para que el mapeo dado por:\n",
        "\n",
        "C=1, E=2, W=3, SE=4\n",
        "\n",
        "sea preferido por el mapeo\n",
        "\n",
        "C=3, E=1, W=2, SE=4\n",
        "\n",
        "o cualquier otra combinación. \n",
        "\n",
        "¿Cómo nos sacamos de encima la arbitrareidad del mapeo?\n",
        "\n",
        "La forma usual es hacer un **one-hot encoding** de los datos. Esto quiere decir, crear una columna nueva para cada valor posible de la variable categorica, y hacer que valga 1 cada fila si tiene el valor que corresponde a la columna. Por ejemplo, creariamos una columna para la variable \"C\" y los dias en los cuales la direccion del viento tiene valor \"C\" tendrian un 1 en esa columna, y 0 en otro caso. \n",
        "\n",
        "Veamos como se hace en scikit-learn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KQgutkQM_vi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52216f91-0f4f-4585-85f5-7de07beb8a72"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "indice = np.logical_not(d['Direccion_Viento_200cm']=='  ') # primero buscamos los valores donde no esta vacio ('  ')\n",
        "\n",
        "d_filt = d[indice] # nos quedamos con esos valores\n",
        "print(d_filt['Direccion_Viento_200cm'].value_counts()) # corroboramos que '  ' se haya ido\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False) # armamos el encoder. sparse=False nos devuelve una matriz comun a la que estamos acostumbrados (en vez de esparsa)\n",
        "direccion = np.array(d_filt['Direccion_Viento_200cm']).reshape(-1, 1) # aca pasamos a un vector la serie de pandas, como es usual\n",
        "encoder.fit(direccion) # fiteo\n",
        "print(encoder.categories_) # estas son las columnas binarias del nuevo encoding\n",
        "direccion_hot = encoder.transform(direccion) # obtenemos la mariz binaria\n",
        "print(direccion_hot)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C     886\n",
            "E     166\n",
            "W      49\n",
            "SE     34\n",
            "Name: Direccion_Viento_200cm, dtype: int64\n",
            "[array(['C ', 'E ', 'SE', 'W '], dtype=object)]\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XlolwchPj2e"
      },
      "source": [
        "# Para llevarse de este notebook\n",
        "\n",
        "*   Vimos que podemos trabajar con un subconjunto de los features en cada uno de los folds. Seleccionamos los mejores K features de acuerdo a un cierto criterio (score F de ANOVA) \n",
        "\n",
        "```\n",
        "    selector = SelectKBest(k=Kbest)\n",
        "    selector.fit(X_train, y_train)\n",
        "    X_train_fs = selector.transform(X_train) \n",
        "    X_test_fs = selector.transform(X_test) \n",
        "```\n",
        "\n",
        "\n",
        "*   Vimos tambien que podemos trabajar con features categoricos haciendo un one-hot encoding, que crea una columna nueva binaria para cada una de los valores de mi variable categorica:\n",
        "\n",
        "\n",
        "```\n",
        "encoder = OneHotEncoder(sparse=False) \n",
        "direccion = np.array(d_filt['Direccion_Viento_200cm']).reshape(-1, 1) \n",
        "encoder.fit(direccion) \n",
        "direccion_hot = encoder.transform(direccion)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wiBEwrEQgMZ"
      },
      "source": [
        "# Preguntas extra\n",
        "\n",
        "*   ¿Se te ocurre una forma en la que podrias seleccionar un subconjunto de features que mejore la performance del clasificador teniendo en cuenta interacciones entre features, pero sin explorar sistematicamente todos los subconjuntos posibles, lo cual casi siempre es imposible computacionalmente?\n",
        "*   Construir un clasificador que tenga en cuenta la direccion del viento (usando one-hot encoding) a ver como impacta eso en el AUC\n",
        "\n"
      ]
    }
  ]
}