{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase 9: Optimización de hiperparámetros.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9GB6QOU9Gcp"
      },
      "source": [
        "# Atacando un problema de clasificación: predicción de días lluviosos (tercera parte)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrGhcfD9U56"
      },
      "source": [
        "# Ingeniería de features\n",
        "\n",
        "... del inglés *feature engineering*. \n",
        "\n",
        "En la jerga de machine learning, las variables (columnas de nuestra matriz X) se denominan *features*. El proceso de *feature engineering* consiste en modificar nuestras features o expandirlas para hacerle el trabajo más fácil al clasificador. Para esto usamos nuestra intuición, por ejemplo, si sabemos que lo importante para un problema de clasificación es el tiempo pasado desde una determinada fecha, entonces podemos referir todos los features temporales en relación a esa fecha. Por supuesto, podría ser que el clasificador *descubriese* por sí mismo esta transformación, pero es más fácil si nos aseguramos de que esté representada claramente en los datos.\n",
        "\n",
        "Una forma típica de aumentar nuestra cantidad de features es agregar nuevos que representen transformaciones no-lineales de features anteriores. Por ejemplo, nuevos features que sean features viejos al cuadrado, al cubo, etc., o bien productos de pares de viejos features. Este tipo de transformación es precisamente la que permite hacer regresión de polinomios.\n",
        "\n",
        "Entonces, vamos a avanzar sobre el problema de detectar días lluviosos haciendo un poco de *feature engineering*.\n",
        "\n",
        "Empezamos como en notebooks anteriores, cargando los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "1CXpKxWm-fgt",
        "outputId": "d384a8dd-eb49-4940-e91e-fca8113d4e40"
      },
      "source": [
        "# Como siempre, tratamos de traer todos los paquetes al ppio\n",
        "from google.colab import drive # Para montar nuestro drive en la consola\n",
        "import matplotlib.pylab as plt # Para gráficos\n",
        "import numpy as np # Para manejo de arrays, operaciones matemáticas, etc.\n",
        "from sklearn.linear_model import LogisticRegression # El método de regresión logística que vamos a usar\n",
        "import pandas as pd # Para manejo de base de datos\n",
        "from sklearn.metrics import confusion_matrix # matriz de confusion\n",
        "\n",
        "# Traemos los datos\n",
        "drive.mount('/content/drive') # Montamos nuestra unidad de Google Drive\n",
        "\n",
        "filename = '/content/drive/My Drive/LaboDatos2021/datosDiariosSanFernandoINTA.xls'\n",
        "\n",
        "d = pd.read_excel(filename) # Levantamos los datos, en este caso, con el método pd.read_excel\n",
        "d.head() # Mostramos las primeras líneas, para darnos una idea"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fecha</th>\n",
              "      <th>Temperatura_Abrigo_150cm</th>\n",
              "      <th>Temperatura_Abrigo_150cm_Maxima</th>\n",
              "      <th>Temperatura_Abrigo_150cm_Minima</th>\n",
              "      <th>Temperatura_Intemperie_5cm_Minima</th>\n",
              "      <th>Temperatura_Intemperie_50cm_Minima</th>\n",
              "      <th>Temperatura_Suelo_5cm_Media</th>\n",
              "      <th>Temperatura_Suelo_10cm_Media</th>\n",
              "      <th>Temperatura_Inte_5cm</th>\n",
              "      <th>Temperatura_Intemperie_150cm_Minima</th>\n",
              "      <th>Humedad_Suelo</th>\n",
              "      <th>Precipitacion_Pluviometrica</th>\n",
              "      <th>Precipitacion_Cronologica</th>\n",
              "      <th>Precipitacion_Maxima_30minutos</th>\n",
              "      <th>Heliofania_Efectiva</th>\n",
              "      <th>Heliofania_Relativa</th>\n",
              "      <th>Tesion_Vapor_Media</th>\n",
              "      <th>Humedad_Media</th>\n",
              "      <th>Humedad_Media_8_14_20</th>\n",
              "      <th>Rocio_Medio</th>\n",
              "      <th>Duracion_Follaje_Mojado</th>\n",
              "      <th>Velocidad_Viento_200cm_Media</th>\n",
              "      <th>Direccion_Viento_200cm</th>\n",
              "      <th>Velocidad_Viento_1000cm_Media</th>\n",
              "      <th>Direccion_Viento_1000cm</th>\n",
              "      <th>Velocidad_Viento_Maxima</th>\n",
              "      <th>Presion_Media</th>\n",
              "      <th>Radiacion_Global</th>\n",
              "      <th>Horas_Frio</th>\n",
              "      <th>Unidades_Frio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-02-08 00:00:00.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>79.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-02-09 00:00:00.0</td>\n",
              "      <td>23.80764</td>\n",
              "      <td>28.6</td>\n",
              "      <td>20.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26.21042</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.75</td>\n",
              "      <td>11.75</td>\n",
              "      <td>5.75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19.25281</td>\n",
              "      <td>66.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>16.462780</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.332639</td>\n",
              "      <td>C</td>\n",
              "      <td>1.665799</td>\n",
              "      <td>C</td>\n",
              "      <td>19.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-23.904030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-02-10 00:00:00.0</td>\n",
              "      <td>24.51389</td>\n",
              "      <td>31.7</td>\n",
              "      <td>19.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26.45972</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>23.28392</td>\n",
              "      <td>72.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>18.622350</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.638195</td>\n",
              "      <td>C</td>\n",
              "      <td>2.047743</td>\n",
              "      <td>C</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-23.904030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-02-11 00:00:00.0</td>\n",
              "      <td>19.50139</td>\n",
              "      <td>24.2</td>\n",
              "      <td>15.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24.66320</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.67725</td>\n",
              "      <td>65.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>12.071650</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.247224</td>\n",
              "      <td>E</td>\n",
              "      <td>4.059030</td>\n",
              "      <td>C</td>\n",
              "      <td>20.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-15.106020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-02-12 00:00:00.0</td>\n",
              "      <td>16.70625</td>\n",
              "      <td>24.6</td>\n",
              "      <td>9.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22.63611</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.13446</td>\n",
              "      <td>66.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>9.627476</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.027778</td>\n",
              "      <td>C</td>\n",
              "      <td>1.284722</td>\n",
              "      <td>C</td>\n",
              "      <td>13.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.394994</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Fecha  Temperatura_Abrigo_150cm  ...  Horas_Frio  Unidades_Frio\n",
              "0  2018-02-08 00:00:00.0                       NaN  ...         NaN            NaN\n",
              "1  2018-02-09 00:00:00.0                  23.80764  ...         0.0     -23.904030\n",
              "2  2018-02-10 00:00:00.0                  24.51389  ...         0.0     -23.904030\n",
              "3  2018-02-11 00:00:00.0                  19.50139  ...         0.0     -15.106020\n",
              "4  2018-02-12 00:00:00.0                  16.70625  ...         0.0      -5.394994\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q344l8n-jnJ"
      },
      "source": [
        "Selecciono un subconjunto de columnas, las renombro y me deshago de los datos faltantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-hbyw5A9FFZ"
      },
      "source": [
        "d_filtrado = d[['Temperatura_Abrigo_150cm_Maxima',\n",
        "                'Temperatura_Abrigo_150cm_Minima',\n",
        "                'Precipitacion_Pluviometrica',\n",
        "                'Velocidad_Viento_Maxima',\n",
        "                'Rocio_Medio',\n",
        "                'Humedad_Media', 'Tesion_Vapor_Media']].dropna().copy() # Nos quedamos con ciertos campos del data set, para facilitar el trabajo. Y para limitarlo.\n",
        "                                                  # Notar que primero aplicamos el método .dropna() para eliminar filas que tengan alguna columna con NaN \n",
        "                                                  # Ademas, el metodo copy() nos asegura que estemos creando un nuevo dataframe \n",
        "d_filtrado.rename({'Temperatura_Abrigo_150cm_Maxima' : 'temperaturaMaxima',\n",
        "                   'Temperatura_Abrigo_150cm_Minima' : 'temperaturaMinima',\n",
        "                   'Precipitacion_Pluviometrica' : 'precipitacion',\n",
        "                   'Humedad_Media' : 'humedad',\n",
        "                   'Rocio_Medio' : 'rocio',\n",
        "                   'Velocidad_Viento_Maxima' : 'viento', 'Tesion_Vapor_Media': 'vapor'},\n",
        "                  axis = 1,\n",
        "                  inplace = True) # Esto toma como input un diccionario en el cual las llaves son los nombres actuales de columnas, y los valores los nombres nuevos (a los que queremos renombrar)\n",
        "                                  # axis = 1 es porque queremos renombrar columnas, y inplace=True es porque queremos \"pisar\" el dataframe al renombrarlo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tScI95c0-qkq"
      },
      "source": [
        "Creo una nueva columna que contiene la etiqueta que indica si llueve o no llueve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHGf6gKH-uiR",
        "outputId": "30b920e0-0dfd-40c9-8811-d0e2b99a0583"
      },
      "source": [
        "d_filtrado['llueveNollueve'] = 0 # empezamos con una columna llena de 0. \n",
        "indice =  d_filtrado['precipitacion'] > 0  # esto me da los valores del indice para los cuales hay precipitacion mayor a 0\n",
        "d_filtrado.loc[indice, 'llueveNollueve'] = 1 # entonces para esos valores del indice pongo 1, porque en el dia correspondiente, llovio\n",
        "\n",
        "d_filtrado['llueveNollueve'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    717\n",
              "1    337\n",
              "Name: llueveNollueve, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB4RJcVT-wmq"
      },
      "source": [
        "Vuelvo a definir la función necesaria para calcular el balanced accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgn_oxjI-7uY"
      },
      "source": [
        "def balanced_accuracy(cm): # funcion para calcular la \"balanced accuracy\"\n",
        "  sensibilidad = cm[1,1]/(cm[1,1]+cm[0,1])\n",
        "  especificidad = cm[0,0]/(cm[1,0]+cm[0,0])\n",
        "  return [sensibilidad, especificidad, (sensibilidad + especificidad)/2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNd5i0D--_Tf"
      },
      "source": [
        "Ahora vamos a usar cinco variables, primero sin ningún tipo de transformación "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0oEC6RO-_fF",
        "outputId": "17956086-49df-45d2-a4f1-57997a5db8f5"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Definamos la matriz X\n",
        "campos = ['temperaturaMinima', 'temperaturaMaxima', 'humedad', 'rocio', 'vapor'] # Lista que contiene las features de interés.\n",
        "X = d_filtrado[campos].values # En este caso no hace falta reshapear, porque ya tiene la forma que queremos\n",
        "print(X.shape) # Como vemos, es una matriz de 1054 filas y 2 columnas\n",
        "y = d_filtrado['llueveNollueve'] # Nuestra etiqueta sigue siende la misma de antes\n",
        "\n",
        "scaler = MinMaxScaler() # primero creo un objeto MinMaxScaler. Por defecto, esto normaliza los datos al intervalo [0,1]\n",
        "scaler.fit(X) # encuentro los parametros para el escaleo\n",
        "X = scaler.transform(X) # aplico la transformacion\n",
        "\n",
        "regLog = LogisticRegression(penalty = 'none', class_weight='balanced', max_iter=10000) # Inicializamos nuevamente el modelo\n",
        "regLog.fit(X, y) # Ajustamos el modelo con los parámetros\n",
        "score = regLog.score(X,y) # Calculamos el score\n",
        "\n",
        "ypred = regLog.predict(X) # con esto obtengo la predicción de las etiquetas en base a mis datos\n",
        "cm = confusion_matrix(ypred, y) # primera entrada son las etiquetas predichas, segunda son las reales\n",
        "metricas = balanced_accuracy(cm)\n",
        "\n",
        "print('El score del modelo es de: {}'.format(round(score,4))) # Le pido que printee el score del modelo. Le pido que lo haga con 4 cifras significativas\n",
        "print('Matriz de confusion del modelo es:')\n",
        "print(cm)\n",
        "print('Sensibilidad del modelo es de: {}'.format(round(metricas[0],4)))  \n",
        "print('Especificidad del modelo es de: {}'.format(round(metricas[1],4)))  \n",
        "print('BA del modelo es de: {}'.format(round(metricas[2],4))) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1054, 5)\n",
            "El score del modelo es de: 0.6632\n",
            "Matriz de confusion del modelo es:\n",
            "[[468 106]\n",
            " [249 231]]\n",
            "Sensibilidad del modelo es de: 0.6855\n",
            "Especificidad del modelo es de: 0.6527\n",
            "BA del modelo es de: 0.6691\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN7xRZun_o-o"
      },
      "source": [
        "Pero ahora vamos a empezar a aumentar el espacio de features, primero agregando los términos cuadráticos y cúbicos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aW_rxM7_zNt",
        "outputId": "67fcecf9-623f-4949-a9b4-f0799cc93e7b"
      },
      "source": [
        "X_1 = np.concatenate((X, X**2,X**3,X**4, X**5), axis=1)\n",
        "\n",
        "scaler = MinMaxScaler() # primero creo un objeto MinMaxScaler. Por defecto, esto normaliza los datos al intervalo [0,1]\n",
        "scaler.fit(X_1) # encuentro los parametros para el escaleo\n",
        "X_1 = scaler.transform(X_1) # aplico la transformacion\n",
        "\n",
        "regLog_1 = LogisticRegression(penalty = 'none', class_weight='balanced', max_iter=10000) # Inicializamos nuevamente el modelo. max_iter es la cantidad de iteraciones maximas del algoritmo de optimizacion de parametros antes de detenerse.\n",
        "regLog_1.fit(X_1, y) # Ajustamos el modelo con los parámetros\n",
        "score = regLog_1.score(X_1,y) # Calculamos el score\n",
        "\n",
        "ypred = regLog_1.predict(X_1) # con esto obtengo la predicción de las etiquetas en base a mis datos\n",
        "cm = confusion_matrix(ypred, y) # primera entrada son las etiquetas predichas, segunda son las reales\n",
        "metricas = balanced_accuracy(cm)\n",
        "\n",
        "print('El score del modelo es de: {}'.format(round(score,4))) # Le pido que printee el score del modelo. Le pido que lo haga con 4 cifras significativas\n",
        "print('Matriz de confusion del modelo es:')\n",
        "print(cm)\n",
        "print('Sensibilidad del modelo es de: {}'.format(round(metricas[0],4)))  \n",
        "print('Especificidad del modelo es de: {}'.format(round(metricas[1],4)))  \n",
        "print('BA del modelo es de: {}'.format(round(metricas[2],4))) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El score del modelo es de: 0.7068\n",
            "Matriz de confusion del modelo es:\n",
            "[[506  98]\n",
            " [211 239]]\n",
            "Sensibilidad del modelo es de: 0.7092\n",
            "Especificidad del modelo es de: 0.7057\n",
            "BA del modelo es de: 0.7075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzWlPvRfHN1d"
      },
      "source": [
        "Tenemos que el BA está aparentemente mejorando.\n",
        "\n",
        "¿Significa que el segundo clasificador tiene mejor performance que el primero?\n",
        "\n",
        "La respuesta es que sí... pero puede ser por sobreajuste. Entonces, ¿cómo hago para averiguar si efectivamente estoy sobreajustando?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Figkos_1HmCU"
      },
      "source": [
        "# Train - test split\n",
        "\n",
        "Una forma de hacer esto es separar parte de los datos para entrenar el modelo (obtener los parámetros) y dejar otra parte para evaluar la performance del modelo. Al hacer eso, tengo las siguientes posibilidades,\n",
        "\n",
        "*   **La performance es baja en el set de entrenamiento y baja en el set de evaluación.** En este caso, necesito o más datos, o más features (puedo probar agregar features que ya tenía transformándolos).\n",
        "\n",
        "*   **La performance es alta en el set de entrenamiento y baja en el set de evaluación.** En este caso, estoy sobreajustando. Puedo intentar resolver esto sumando más datos a mi modelo, o bien aplicando regularización.\n",
        "\n",
        "*   **La performance es alta en el set de entrenamiento y alta en el set de evaluación.** El caso ideal: aprendí parámetros que me sirven para generalizar a datos nuevos sin sobreajustar.\n",
        "\n",
        "Podemos usar:\n",
        "\n",
        "```\n",
        "sklearn.model_selection.train_test_split\n",
        "```\n",
        "\n",
        "para separar los datos en partes de entrenamiento y de evaluación, de forma aleatoria.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtFdNvnhIlaW",
        "outputId": "5ac28c6d-8b15-4e91-fd22-25f3f60e0508"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "scaler = MinMaxScaler() # primero creo un objeto MinMaxScaler. Por defecto, esto normaliza los datos al intervalo [0,1]\n",
        "scaler.fit(X_train) # encuentro los parametros para el escaleo\n",
        "X_train = scaler.transform(X_train) # aplico la transformacion\n",
        "\n",
        "scaler = MinMaxScaler() # primero creo un objeto MinMaxScaler. Por defecto, esto normaliza los datos al intervalo [0,1]\n",
        "scaler.fit(X_test) # encuentro los parametros para el escaleo\n",
        "X_test = scaler.transform(X_test) # aplico la transformacion\n",
        "\n",
        "#### IMPORTANTE: HAGO LA NORMALIZACION DE LOS DATOS POR SEPARADO EN LOS SETS DE TRAIN Y TEST\n",
        "\n",
        "regLog = LogisticRegression(penalty = 'none', class_weight='balanced', max_iter=10000) # Inicializamos nuevamente el modelo\n",
        "regLog.fit(X_train, y_train) # Ajustamos el modelo con los parámetros\n",
        "\n",
        "score_train = regLog.score(X_train,y_train) # Calculamos el score\n",
        "ypred_train = regLog.predict(X_train) # con esto obtengo la predicción de las etiquetas en base a mis datos\n",
        "cm_train = confusion_matrix(ypred_train, y_train) # primera entrada son las etiquetas predichas, segunda son las reales\n",
        "metricas_train = balanced_accuracy(cm_train)\n",
        "\n",
        "print('Para TRAIN set:')\n",
        "print('El score del modelo es de: {}'.format(round(score_train,4))) # Le pido que printee el score del modelo. Le pido que lo haga con 4 cifras significativas\n",
        "print('Matriz de confusion del modelo es:')\n",
        "print(cm_train)\n",
        "print('Sensibilidad del modelo es de: {}'.format(round(metricas_train[0],4)))  \n",
        "print('Especificidad del modelo es de: {}'.format(round(metricas_train[1],4)))  \n",
        "print('BA del modelo es de: {}'.format(round(metricas_train[2],4))) \n",
        "print('\\n')\n",
        "\n",
        "score_test= regLog.score(X_test,y_test) # Calculamos el score\n",
        "ypred_test= regLog.predict(X_test) # con esto obtengo la predicción de las etiquetas en base a mis datos\n",
        "cm_test = confusion_matrix(ypred_test, y_test) # primera entrada son las etiquetas predichas, segunda son las reales\n",
        "metricas_test = balanced_accuracy(cm_test)\n",
        "\n",
        "\n",
        "print('Para TEST set:')\n",
        "print('El score del modelo es de: {}'.format(round(score_test,4))) # Le pido que printee el score del modelo. Le pido que lo haga con 4 cifras significativas\n",
        "print('Matriz de confusion del modelo es:')\n",
        "print(cm_test)\n",
        "print('Sensibilidad del modelo es de: {}'.format(round(metricas_test[0],4)))  \n",
        "print('Especificidad del modelo es de: {}'.format(round(metricas_test[1],4)))  \n",
        "print('BA del modelo es de: {}'.format(round(metricas_test[2],4))) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Para TRAIN set:\n",
            "El score del modelo es de: 0.6608\n",
            "Matriz de confusion del modelo es:\n",
            "[[335  73]\n",
            " [177 152]]\n",
            "Sensibilidad del modelo es de: 0.6756\n",
            "Especificidad del modelo es de: 0.6543\n",
            "BA del modelo es de: 0.6649\n",
            "\n",
            "\n",
            "Para TEST set:\n",
            "El score del modelo es de: 0.7161\n",
            "Matriz de confusion del modelo es:\n",
            "[[182  67]\n",
            " [ 23  45]]\n",
            "Sensibilidad del modelo es de: 0.4018\n",
            "Especificidad del modelo es de: 0.8878\n",
            "BA del modelo es de: 0.6448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSO8LLDtKqpe"
      },
      "source": [
        "Vemos que en este caso en general tendremos peores performance en el test set (ojo con las fluctuaciones estocasticas por al armar el train-test split)\n",
        "\n",
        "¿Y si vamos al caso con features polinómicos agregados?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW0_Di1JK3yK",
        "outputId": "c98f4183-e788-41dd-f9ef-4be18a7310cb"
      },
      "source": [
        "# Definamos la matriz X\n",
        "campos = ['temperaturaMinima', 'temperaturaMaxima', 'humedad', 'rocio', 'vapor'] # Lista que contiene las features de interés.\n",
        "X = d_filtrado[campos].values # En este caso no hace falta reshapear, porque ya tiene la forma que queremos\n",
        "\n",
        "X_temp = X\n",
        "for i in np.arange(2,5):\n",
        "    X_temp = np.concatenate((X_temp,X**i), axis=1)\n",
        "X_1 = X_temp\n",
        " \n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_1, y, test_size=0.3)\n",
        "\n",
        "scaler = MinMaxScaler() # primero creo un objeto MinMaxScaler. Por defecto, esto normaliza los datos al intervalo [0,1]\n",
        "scaler.fit(X_train) # encuentro los parametros para el escaleo\n",
        "X_train = scaler.transform(X_train) # aplico la transformacion\n",
        "\n",
        "scaler = MinMaxScaler() # primero creo un objeto MinMaxScaler. Por defecto, esto normaliza los datos al intervalo [0,1]\n",
        "scaler.fit(X_test) # encuentro los parametros para el escaleo\n",
        "X_test = scaler.transform(X_test) # aplico la transformacion\n",
        "\n",
        "regLog_1 = LogisticRegression(penalty = 'none', class_weight='balanced', max_iter=50000) # Inicializamos nuevamente el modelo\n",
        "regLog_1.fit(X_train, y_train) # Ajustamos el modelo con los parámetros\n",
        "\n",
        "score_train = regLog_1.score(X_train,y_train) # Calculamos el score\n",
        "ypred_train = regLog_1.predict(X_train) # con esto obtengo la predicción de las etiquetas en base a mis datos\n",
        "cm_train = confusion_matrix(ypred_train, y_train) # primera entrada son las etiquetas predichas, segunda son las reales\n",
        "metricas_train = balanced_accuracy(cm_train)\n",
        "\n",
        "print('Para TRAIN set:')\n",
        "print('El score del modelo es de: {}'.format(round(score_train,4))) # Le pido que printee el score del modelo. Le pido que lo haga con 4 cifras significativas\n",
        "print('Matriz de confusion del modelo es:')\n",
        "print(cm_train)\n",
        "print('Sensibilidad del modelo es de: {}'.format(round(metricas_train[0],4)))  \n",
        "print('Especificidad del modelo es de: {}'.format(round(metricas_train[1],4)))  \n",
        "print('BA del modelo es de: {}'.format(round(metricas_train[2],4))) \n",
        "print('\\n')\n",
        "\n",
        "score_test= regLog_1.score(X_test,y_test) # Calculamos el score\n",
        "ypred_test= regLog_1.predict(X_test) # con esto obtengo la predicción de las etiquetas en base a mis datos\n",
        "cm_test = confusion_matrix(ypred_test, y_test) # primera entrada son las etiquetas predichas, segunda son las reales\n",
        "metricas_test = balanced_accuracy(cm_test)\n",
        "\n",
        "\n",
        "print('Para TEST set:')\n",
        "print('El score del modelo es de: {}'.format(round(score_test,4))) # Le pido que printee el score del modelo. Le pido que lo haga con 4 cifras significativas\n",
        "print('Matriz de confusion del modelo es:')\n",
        "print(cm_test)\n",
        "print('Sensibilidad del modelo es de: {}'.format(round(metricas_test[0],4)))  \n",
        "print('Especificidad del modelo es de: {}'.format(round(metricas_test[1],4)))  \n",
        "print('BA del modelo es de: {}'.format(round(metricas_test[2],4))) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Para TRAIN set:\n",
            "El score del modelo es de: 0.7191\n",
            "Matriz de confusion del modelo es:\n",
            "[[358  78]\n",
            " [129 172]]\n",
            "Sensibilidad del modelo es de: 0.688\n",
            "Especificidad del modelo es de: 0.7351\n",
            "BA del modelo es de: 0.7116\n",
            "\n",
            "\n",
            "Para TEST set:\n",
            "El score del modelo es de: 0.7224\n",
            "Matriz de confusion del modelo es:\n",
            "[[229  87]\n",
            " [  1   0]]\n",
            "Sensibilidad del modelo es de: 0.0\n",
            "Especificidad del modelo es de: 0.9957\n",
            "BA del modelo es de: 0.4978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22hbslJVLkPE"
      },
      "source": [
        "Esto es todavia peor en el test set. Señal de que estamos sobreajustando los datos.\n",
        "\n",
        "Veamos si mejora algo la regularización..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJezK8sALnRz",
        "outputId": "3d50f2c4-070e-431a-9121-aec4a42ebee4"
      },
      "source": [
        "# Definamos la matriz X\n",
        "campos = ['temperaturaMinima', 'temperaturaMaxima', 'humedad', 'rocio', 'vapor'] # Lista que contiene las features de interés.\n",
        "X = d_filtrado[campos].values # En este caso no hace falta reshapear, porque ya tiene la forma que queremos\n",
        "\n",
        "\n",
        "X_temp = X\n",
        "for i in np.arange(2,5):\n",
        "    X_temp = np.concatenate((X_temp,X**i), axis=1)\n",
        "X_1 = X_temp\n",
        " \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_1, y, test_size=0.3)\n",
        "\n",
        "scaler = MinMaxScaler() # primero creo un objeto MinMaxScaler. Por defecto, esto normaliza los datos al intervalo [0,1]\n",
        "scaler.fit(X_train) # encuentro los parametros para el escaleo\n",
        "X_train = scaler.transform(X_train) # aplico la transformacion\n",
        "\n",
        "scaler = MinMaxScaler() # primero creo un objeto MinMaxScaler. Por defecto, esto normaliza los datos al intervalo [0,1]\n",
        "scaler.fit(X_test) # encuentro los parametros para el escaleo\n",
        "X_test = scaler.transform(X_test) # aplico la transformacion\n",
        "\n",
        "regLog_1 = LogisticRegression(penalty = 'l2', class_weight='balanced', max_iter=10000) # Inicializamos nuevamente el modelo\n",
        "regLog_1.fit(X_train, y_train) # Ajustamos el modelo con los parámetros\n",
        "\n",
        "score_train = regLog_1.score(X_train,y_train) # Calculamos el score\n",
        "ypred_train = regLog_1.predict(X_train) # con esto obtengo la predicción de las etiquetas en base a mis datos\n",
        "cm_train = confusion_matrix(ypred_train, y_train) # primera entrada son las etiquetas predichas, segunda son las reales\n",
        "metricas_train = balanced_accuracy(cm_train)\n",
        "\n",
        "print('Para TRAIN set:')\n",
        "print('El score del modelo es de: {}'.format(round(score_train,4))) # Le pido que printee el score del modelo. Le pido que lo haga con 4 cifras significativas\n",
        "print('Matriz de confusion del modelo es:')\n",
        "print(cm_train)\n",
        "print('Sensibilidad del modelo es de: {}'.format(round(metricas_train[0],4)))  \n",
        "print('Especificidad del modelo es de: {}'.format(round(metricas_train[1],4)))  \n",
        "print('BA del modelo es de: {}'.format(round(metricas_train[2],4))) \n",
        "print('\\n')\n",
        "\n",
        "score_test= regLog_1.score(X_test,y_test) # Calculamos el score\n",
        "ypred_test= regLog_1.predict(X_test) # con esto obtengo la predicción de las etiquetas en base a mis datos\n",
        "cm_test = confusion_matrix(ypred_test, y_test) # primera entrada son las etiquetas predichas, segunda son las reales\n",
        "metricas_test = balanced_accuracy(cm_test)\n",
        "\n",
        "\n",
        "print('Para TEST set:')\n",
        "print('El score del modelo es de: {}'.format(round(score_test,4))) # Le pido que printee el score del modelo. Le pido que lo haga con 4 cifras significativas\n",
        "print('Matriz de confusion del modelo es:')\n",
        "print(cm_test)\n",
        "print('Sensibilidad del modelo es de: {}'.format(round(metricas_test[0],4)))  \n",
        "print('Especificidad del modelo es de: {}'.format(round(metricas_test[1],4)))  \n",
        "print('BA del modelo es de: {}'.format(round(metricas_test[2],4))) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Para TRAIN set:\n",
            "El score del modelo es de: 0.6974\n",
            "Matriz de confusion del modelo es:\n",
            "[[356  83]\n",
            " [140 158]]\n",
            "Sensibilidad del modelo es de: 0.6556\n",
            "Especificidad del modelo es de: 0.7177\n",
            "BA del modelo es de: 0.6867\n",
            "\n",
            "\n",
            "Para TEST set:\n",
            "El score del modelo es de: 0.6246\n",
            "Matriz de confusion del modelo es:\n",
            "[[117  15]\n",
            " [104  81]]\n",
            "Sensibilidad del modelo es de: 0.8438\n",
            "Especificidad del modelo es de: 0.5294\n",
            "BA del modelo es de: 0.6866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt3mN6rJL2ZU"
      },
      "source": [
        "# Optimización de hiperparámetros\n",
        "\n",
        "Vimos que al sumar regularización, la performance del modelo de repente se ve incrementada en el test set. Posiblemente esto dependa del valor de la constante de penalización C que usamos para la regresión. **Esto es lo que llamamos un hiperparámetro del modelo.** Otro hiperparámetro posible es el grado de la potencia máxima en la que aparecen los features.\n",
        "\n",
        "Entonces tiene sentido hacer la siguiente pregunta: ¿para qué elección de hiperparámetros **maximizo** la performance del clasificador en el dataset de evaluación (test)?\n",
        "\n",
        "Vamos a proponer la siguiente forma de abordar este problema:\n",
        "\n",
        "1.   Generamos una función que entrene y evalue el clasificador usando un train-test split, que incorpore como input los hiperparámetros\n",
        "2.   Para cada elección del par de hiperparámetros, repetimos este procedimiento unas cuantas (>100) veces y nos quedamos con la performance promedio. Esto es necesario porque el train-test split es aleatorio.\n",
        "3.   Repetimos para todas las combinaciones de nuestros hiperparámetros y encontramos aquella que maximiza la performance del clasificador.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmQlOdxvNGog"
      },
      "source": [
        "def classifier(X,y,n,c):  # X es la matriz de features, y los labels, n-1 la potencia máxima en la que aparecerán los features, C es la constante de regularización\n",
        "\n",
        "  X_temp = X\n",
        "  for i in np.arange(2,n):\n",
        "    X_temp = np.concatenate((X_temp,X**i), axis=1)\n",
        "  X = X_temp\n",
        " \n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "  scaler = MinMaxScaler() # primero creo un objeto MinMaxScaler. Por defecto, esto normaliza los datos al intervalo [0,1]\n",
        "  scaler.fit(X_train) # encuentro los parametros para el escaleo\n",
        "  X_train = scaler.transform(X_train) # aplico la transformacion\n",
        "\n",
        "  scaler = MinMaxScaler() # primero creo un objeto MinMaxScaler. Por defecto, esto normaliza los datos al intervalo [0,1]\n",
        "  scaler.fit(X_test) # encuentro los parametros para el escaleo\n",
        "  X_test = scaler.transform(X_test) # aplico la transformacion\n",
        "\n",
        "  regLog = LogisticRegression(penalty = 'l2', class_weight='balanced', C=c, max_iter=10000) \n",
        "  regLog.fit(X_train, y_train) \n",
        "  \n",
        "  ypred_train = regLog.predict(X_train) # con esto obtengo la predicción de las etiquetas en base a mis datos\n",
        "  cm_train = confusion_matrix(ypred_train, y_train) # primera entrada son las etiquetas predichas, segunda son las reales\n",
        "  metricas_train = balanced_accuracy(cm_train)\n",
        "\n",
        "  ypred_test = regLog.predict(X_test) # con esto obtengo la predicción de las etiquetas en base a mis datos\n",
        "  cm_test= confusion_matrix(ypred_test, y_test) # primera entrada son las etiquetas predichas, segunda son las reales\n",
        "  metricas_test = balanced_accuracy(cm_test)\n",
        "\n",
        "  return metricas_train[2], metricas_test[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rarcHWDwObEC"
      },
      "source": [
        "Probemos algunos casos particulares y veamos como depende del azar,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB3U3gmKNeHZ",
        "outputId": "bf8efbb8-02d5-4586-d825-f3f5fd74057d"
      },
      "source": [
        " # repitamos varias veces para ver como cambia\n",
        "\n",
        "campos = ['temperaturaMinima', 'temperaturaMaxima', 'humedad', 'rocio', 'vapor'] # Lista que contiene las features de interés.\n",
        "X = d_filtrado[campos].values # En este caso no hace falta reshapear, porque ya tiene la forma que queremos\n",
        "y = d_filtrado['llueveNollueve'] # Nuestra etiqueta sigue siende la misma de antes\n",
        "\n",
        "\n",
        "BA_train, BA_test = classifier(X,y,6,1)  \n",
        "print(BA_train, BA_test)\n",
        "\n",
        "# aumento C\n",
        "\n",
        "BA_train, BA_test = classifier(X,y,6,1000)  \n",
        "print(BA_train, BA_test)\n",
        "\n",
        "# disminuyo C\n",
        "\n",
        "BA_train, BA_test = classifier(X,y,6,0.0001)  \n",
        "print(BA_train, BA_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6939156161564188 0.6710280373831776\n",
            "0.7059943401080524 0.6380284824210058\n",
            "0.6382570101211216 0.6868115942028985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM221a5zPVl_"
      },
      "source": [
        "Claramente es difícil sacar conclusiones. Así que vamos a recorrer sistemáticamente los valores de una grilla, replicando muchas veces en cada combinación.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy5XMD3sPpjN",
        "outputId": "e15a97d2-d9f4-4a0b-ab7b-2a21c8c9e5a5"
      },
      "source": [
        "campos = ['temperaturaMinima', 'temperaturaMaxima', 'humedad', 'rocio', 'vapor'] # Lista que contiene las features de interés.\n",
        "X = d_filtrado[campos].values # En este caso no hace falta reshapear, porque ya tiene la forma que queremos\n",
        "y = d_filtrado['llueveNollueve'] # Nuestra etiqueta sigue siende la misma de antes\n",
        "\n",
        "n_values = np.arange(1,7) # rango de potencias maximas \n",
        "c_values = np.arange(0.2,5,0.2) # rango de constantes de regularizacion\n",
        "\n",
        "iterations = 500 # esto se puede poner en 100 para que no tarde mil años durante la clase\n",
        "\n",
        "mean_BA_train = np.zeros((len(n_values),len(c_values))) #  los arrays con los balanced accuracy para cada combinacion de hiperparametros, para train y test\n",
        "mean_BA_test = np.zeros((len(n_values),len(c_values)))\n",
        "\n",
        "for xx,n in enumerate(n_values): # itero sobre n\n",
        "    for yy,c in enumerate(c_values): # itero sobre c\n",
        "      BA_test_temp = [] # variables temporales para ir poniendo los resultados a lo largo de las iteraciones\n",
        "      BA_train_temp = []\n",
        "      print([n,c]) # esto es para los ansiosos\n",
        "      for it in np.arange(iterations): # itero\n",
        "          BA_train, BA_test = classifier(X,y,n,c)  # calculo las balanced accuracy para esos hiperparametros\n",
        "          BA_test_temp.append(BA_test) # agrego a las variables temporales\n",
        "          BA_train_temp.append(BA_train)\n",
        "      mean_BA_train[xx,yy] = np.mean(BA_train_temp) # calculo promedios\n",
        "      mean_BA_test[xx,yy] = np.mean(BA_test_temp)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4 5 6]\n",
            "[0.2 0.4 0.6 0.8 1.  1.2 1.4 1.6 1.8 2.  2.2 2.4 2.6 2.8 3.  3.2 3.4 3.6\n",
            " 3.8 4.  4.2 4.4 4.6 4.8]\n",
            "[0, 0]\n",
            "[0, 1]\n",
            "[0, 2]\n",
            "[0, 3]\n",
            "[0, 4]\n",
            "[0, 5]\n",
            "[0, 6]\n",
            "[0, 7]\n",
            "[0, 8]\n",
            "[0, 9]\n",
            "[0, 10]\n",
            "[0, 11]\n",
            "[0, 12]\n",
            "[0, 13]\n",
            "[0, 14]\n",
            "[0, 15]\n",
            "[0, 16]\n",
            "[0, 17]\n",
            "[0, 18]\n",
            "[0, 19]\n",
            "[0, 20]\n",
            "[0, 21]\n",
            "[0, 22]\n",
            "[0, 23]\n",
            "[1, 0]\n",
            "[1, 1]\n",
            "[1, 2]\n",
            "[1, 3]\n",
            "[1, 4]\n",
            "[1, 5]\n",
            "[1, 6]\n",
            "[1, 7]\n",
            "[1, 8]\n",
            "[1, 9]\n",
            "[1, 10]\n",
            "[1, 11]\n",
            "[1, 12]\n",
            "[1, 13]\n",
            "[1, 14]\n",
            "[1, 15]\n",
            "[1, 16]\n",
            "[1, 17]\n",
            "[1, 18]\n",
            "[1, 19]\n",
            "[1, 20]\n",
            "[1, 21]\n",
            "[1, 22]\n",
            "[1, 23]\n",
            "[2, 0]\n",
            "[2, 1]\n",
            "[2, 2]\n",
            "[2, 3]\n",
            "[2, 4]\n",
            "[2, 5]\n",
            "[2, 6]\n",
            "[2, 7]\n",
            "[2, 8]\n",
            "[2, 9]\n",
            "[2, 10]\n",
            "[2, 11]\n",
            "[2, 12]\n",
            "[2, 13]\n",
            "[2, 14]\n",
            "[2, 15]\n",
            "[2, 16]\n",
            "[2, 17]\n",
            "[2, 18]\n",
            "[2, 19]\n",
            "[2, 20]\n",
            "[2, 21]\n",
            "[2, 22]\n",
            "[2, 23]\n",
            "[3, 0]\n",
            "[3, 1]\n",
            "[3, 2]\n",
            "[3, 3]\n",
            "[3, 4]\n",
            "[3, 5]\n",
            "[3, 6]\n",
            "[3, 7]\n",
            "[3, 8]\n",
            "[3, 9]\n",
            "[3, 10]\n",
            "[3, 11]\n",
            "[3, 12]\n",
            "[3, 13]\n",
            "[3, 14]\n",
            "[3, 15]\n",
            "[3, 16]\n",
            "[3, 17]\n",
            "[3, 18]\n",
            "[3, 19]\n",
            "[3, 20]\n",
            "[3, 21]\n",
            "[3, 22]\n",
            "[3, 23]\n",
            "[4, 0]\n",
            "[4, 1]\n",
            "[4, 2]\n",
            "[4, 3]\n",
            "[4, 4]\n",
            "[4, 5]\n",
            "[4, 6]\n",
            "[4, 7]\n",
            "[4, 8]\n",
            "[4, 9]\n",
            "[4, 10]\n",
            "[4, 11]\n",
            "[4, 12]\n",
            "[4, 13]\n",
            "[4, 14]\n",
            "[4, 15]\n",
            "[4, 16]\n",
            "[4, 17]\n",
            "[4, 18]\n",
            "[4, 19]\n",
            "[4, 20]\n",
            "[4, 21]\n",
            "[4, 22]\n",
            "[4, 23]\n",
            "[5, 0]\n",
            "[5, 1]\n",
            "[5, 2]\n",
            "[5, 3]\n",
            "[5, 4]\n",
            "[5, 5]\n",
            "[5, 6]\n",
            "[5, 7]\n",
            "[5, 8]\n",
            "[5, 9]\n",
            "[5, 10]\n",
            "[5, 11]\n",
            "[5, 12]\n",
            "[5, 13]\n",
            "[5, 14]\n",
            "[5, 15]\n",
            "[5, 16]\n",
            "[5, 17]\n",
            "[5, 18]\n",
            "[5, 19]\n",
            "[5, 20]\n",
            "[5, 21]\n",
            "[5, 22]\n",
            "[5, 23]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUihtt8BM5Ho"
      },
      "source": [
        "Por último, grafico como me quedan las BA para cada combinación de hiperparametros, y puedo encontrar cual es la BA máxima para cada caso, y con qué hiperparámetros la obtengo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        },
        "id": "bNNGr6HzQt7Y",
        "outputId": "d07f42ae-dba8-4625-ad62-6573d224b1cb"
      },
      "source": [
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,8))  # visualizacion train set\n",
        "img = ax.imshow(mean_BA_train) # graficamos la matriz\n",
        "y_label_list = n_values # estos son los ticks de los ejes\n",
        "x_label_list = c_values\n",
        "ax.set_xticks(np.arange(len(c_values))) # creo los ticks de los ejes\n",
        "ax.set_yticks(np.arange(len(n_values)))\n",
        "ax.set_yticklabels(y_label_list) # pongo los ticks de los ejes\n",
        "ax.set_xticklabels(x_label_list)\n",
        "plt.xlabel('C (constante de regularizacion)', fontsize=15) # le pongo nombre a ejes y titulo \n",
        "plt.ylabel('n (potencia maxima para elevar)', fontsize=15)\n",
        "plt.title('BA (train set)', fontsize=20)\n",
        "fig.colorbar(img, orientation=\"horizontal\", pad=0.2) # la barra de colores donde no me moleste\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,8)) # visualizacion test set, ver comentarios para el caso train, es idéntico\n",
        "img = ax.imshow(mean_BA_test)\n",
        "y_label_list = n_values\n",
        "x_label_list = c_values\n",
        "ax.set_xticks(np.arange(len(c_values)))\n",
        "ax.set_yticks(np.arange(len(n_values)))\n",
        "ax.set_yticklabels(y_label_list)\n",
        "ax.set_xticklabels(x_label_list)\n",
        "plt.xlabel('C (constante de regularizacion)', fontsize=15)\n",
        "plt.ylabel('n (potencia maxima para elevar)', fontsize=15)\n",
        "plt.title('BA (test set)', fontsize=20)\n",
        "fig.colorbar(img, orientation=\"horizontal\", pad=0.2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7fc16165a850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 230
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oY49Rj9NVz4"
      },
      "source": [
        "Vemos entonces que en el caso de train set, como es esperado, aumenta la performance a medida que sumo potencias de grado mayor de los features.\n",
        "\n",
        "En el caso del test set no es tan simple. Necesito ademas que la constante de regularizacion tenga un valor cercano a 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDuKAdHcOd61"
      },
      "source": [
        "# Para llevarse de este notebook\n",
        "\n",
        "*   A la hora de evaluar si hay sobreajuste, es clave armar un train-test split, entrenar en el primero y evaluar en el segundo:\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "```\n",
        "\n",
        "Como este split es al azar, se recomienda hacerlo muchas veces y luego promediar las accuracies resultantes para tener una buena idea de la performance del clasificador en ambos grupos.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ka8I69vNjMm"
      },
      "source": [
        "# Preguntas extra\n",
        "\n",
        "*   **Estimamos la performance como el máximo de la matriz de balanced accuracies para el test set. Elegimos la combinación de hiperparámetros que resulta en ese máximo. Ahora vamos a aplicar el clasificador a un nuevo subconjunto de datos, y obtenemos una performance peor de lo que esperábamos. ¿Por qué?**\n",
        "*   Probar agregando otras features, por ejemplo, términos cruzados, y ver cómo cambia la performance. \n",
        "*   Esta vez usamos cinco features. ¿Es más mejor? ¿Puedo aumentar la performance si borro features que no son importantes?\n",
        "*   Si tengo 5 features, en principio hay 2^5 posibles clasificadores usando todos los subconjuntos de features posibles. Claramente, a medida que aumenta la cantidad de features, hacer una evaluacion exhaustiva se vuelve imposible. Este es el problema de *seleccion de features* ¿Se te ocurra alguna idea sobre cómo abordarlo si la cantidad de features es muy grande?\n",
        "\n"
      ]
    }
  ]
}